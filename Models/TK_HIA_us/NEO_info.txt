(Protocosas) lauri@ProtoALIEN:~/Desktop/ProtoQSAR/generate_models/NEO$ python NEO.py 

#########################################################################
######################### WELCOME TO NEO script #########################
#########################################################################
This script will allow you to: 
 	- eliminate 3D descriptors
 	- "y" transformation
 	- perform the initial unsupervised feature reduction
 	- perform the train/test split based on kmeans
 	- descriptor standarization
 	- select the relevant features based on:
		 · Recursive feature elimination (RFE)
		 · Feature importance (FI) based on Ligth gradient boosting machine (LGBM)
		 · Permutation importance (PI)
 	- select your own features features

Please input your PATH (enter to: "../data/Af_MIC80_definitva/no3D/OWNdesc/"): /home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/
Please input your MODEL NAME (enter to: Af_MIC80_no3D): TK_HIA_us
######################### MAIN MENU #########################

Please select what do you want to do: 
[01] Elimination of 3D descriptors [your dataset will be saved as [Name]_no3D]
[1] "y" transformation + dataset random order + Knn imputation
[2] Initial feature reduction: infinite, correlated, constant and empty values
[3] Generation of train and test sets based in kmeans
[4] Descriptor standarization
[5] Feature selection by RFE
[6] Feature selection by FI based on LGBM
[7] Feature selection by Permutation importance
[8] Select own features (inside the script)
[0] Exit NEO

Your choice: 1
This part of the code will do the y" transformation, randomization of the dataset order and  Knn imputation.
From this version of NEO is its compulsory to perform the inputation here, as this will create the un-imputed file needed for reimputation.
[+] "y" transformation
A file located in "/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/" folder is needed
This file must be called: "TK_HIA_us-paralel_calculated_with_y.csv"
Continue (Y/n)?

Please select your type of model: 
[1] Regression
[2] Classification
Your choice: 2

I am so sorry, there is nothing yet for your request. Please try tomorrow with more coffee and cookies.
[+] dataset random sort

	The following file has been created (save it as you will need it for feature reduction):

	/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-calculated_preimputation.csv
[+] dataset imputation
Size of the database, preimputation: (395, 977)
[+] fitting
[+] transforming
Size of the database, postimputation: (395, 977)
     C-001  C-002  C-003  C-004  C-005  C-006  C-007  C-008  C-009  C-010  C-011  C-012  ...  NssssN  NaaO  NssS  NaaS  NdssS   SssCH2   SsssCH    SdssC    SaasC    SaaaC   SssssC    SLogP
0      1.0    3.0    0.0    1.0    0.0    4.0    0.0    1.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  5.07431 -0.44716 -0.11095  2.00656  0.00000 -0.54979  3.70700
1      0.0    4.0    0.0    0.0    0.0    4.0    0.0    6.0    1.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0 -1.97309 -9.91344 -8.71327  0.00000  0.00000  0.00000 -8.67313
2      0.0    0.0    0.0    0.0    0.0    6.0    0.0    0.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  4.96631  0.00000 -0.05562  1.27893  0.00000  0.00000  1.40200
3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  0.00000  0.00000 -2.09028  0.00000  0.00000  0.00000 -0.15790
4      1.0    2.0    0.0    0.0    1.0    2.0    0.0    1.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  1.73565 -0.85517 -0.97673  2.79786  0.00000  0.00000  2.67870
..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ...     ...   ...   ...   ...    ...      ...      ...      ...      ...      ...      ...      ...
390    1.0    2.0    1.0    0.0    1.0    2.0    0.0    2.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  2.39438 -0.03671  1.25869  2.59480  2.45246  0.00000  1.92470
391    0.0    0.0    0.0    0.0    3.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  0.00000  0.00000 -0.36926  0.54509  0.00000  0.00000  0.57150
392    5.0    7.0    4.0    2.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  7.06131  1.94304  1.29104  0.00000  0.00000 -0.00114  5.10100
393    0.0    3.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  ...     0.0   1.0   0.0   0.0    0.0  3.61079  0.00000 -0.34303  3.11975  0.55987  0.00000  4.62710
394    2.0    2.0    2.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  ...     0.0   0.0   0.0   0.0    0.0  1.58528  0.67630 -0.74898  0.00000  0.00000  0.00000  1.08210

[395 rows x 977 columns]

The following files have been created:

/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-calculated_imputed_ytransformed.csv

Do you want to perform any other step?(y/n):  
######################### MAIN MENU #########################

Please select what do you want to do: 
[01] Elimination of 3D descriptors [your dataset will be saved as [Name]_no3D]
[1] "y" transformation + dataset random order + Knn imputation
[2] Initial feature reduction: infinite, correlated, constant and empty values
[3] Generation of train and test sets based in kmeans
[4] Descriptor standarization
[5] Feature selection by RFE
[6] Feature selection by FI based on LGBM
[7] Feature selection by Permutation importance
[8] Select own features (inside the script)
[0] Exit NEO

Your choice: 2
A file located in "/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/" folder is needed
This file must be called: "TK_HIA_us-calculated_imputed_ytransformed.csv"
Continue (Y/n)?
[1] Initial feature reduction: infinite, correlated, constant and empty values
0 infinite values
0 features with greater than 0.00 missing values.

79 features with a correlation magnitude greater than 0.90.

22 features with a single unique value.

Data has not been one-hot encoded
Removed 101 features including one-hot features.

The following files have been created:

/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-initial_reduction.csv

Do you want to perform any other step?(y/n):  
######################### MAIN MENU #########################

Please select what do you want to do: 
[01] Elimination of 3D descriptors [your dataset will be saved as [Name]_no3D]
[1] "y" transformation + dataset random order + Knn imputation
[2] Initial feature reduction: infinite, correlated, constant and empty values
[3] Generation of train and test sets based in kmeans
[4] Descriptor standarization
[5] Feature selection by RFE
[6] Feature selection by FI based on LGBM
[7] Feature selection by Permutation importance
[8] Select own features (inside the script)
[0] Exit NEO

Your choice: 3

Please select your type of model: 
[1] Regression
[2] Classification
Your choice (1/2)?: 2
Please input your desired TEST SIZE (enter to: "0.25"): 0.25
A file located in "/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/" folder is needed
This file must be called: "TK_HIA_us-initial_reduction.csv"
Continue (Y/n)?
[+] Generation of train and test sets based in kmeans

OPTIMAL NUMBER OF CLUSTERS:  4

NUMBER OF CLUSTERS:  4
	SETS:  {376, 2, 12, 5}
	ALERTS!!
		compound number:  107 
			SMILE:  NC1C(=O)NC2Cc3ccc(c(Cl)c3)Oc3cc4cc(c3O)Oc3ccc(cc3Cl)C(O)C3NC(=O)C(NC(=O)C4NC(=O)C(NC2=O)c2cc(O)cc(c2)Oc2cc1ccc2O)c1ccc(O)c(c1)-c1c(O)cc(O)cc1C(C(=O)O)NC3=O 0.0
		compound number:  116 
			SMILE:  CN[C@H](CC(C)C)C(=O)N[C@@H]1C(=O)N[C@@H](CC(N)=O)C(=O)N[C@@H]2C(=O)N[C@H]3C(=O)N[C@H](C(=O)N[C@H](C(=O)O)c4cc(O)cc(O)c4-c4cc3ccc4O)[C@@H](O)c3ccc(c(Cl)c3)Oc3cc2cc(c3O[C@@H]2O[C@H](CO)[C@H](O)[C@@H](O)[C@H]2O[C@@H]2C[C@](C)(N)[C@@H](O)[C@H](C)O2)Oc2ccc(cc2Cl)[C@H]1O 0.0
[107, 116]
you have some molecular alerts. It means that these molecules are quite dissimilar
You can (1) eliminate them or (2) maintain them
What is your choice (1/2)?2
Ok, continue with entire dataframe.

NUMBER OF CLUSTERS:  3
	SETS:  {2, 380, 13}
	ALERTS!!
		compound number:  107 
			SMILE:  NC1C(=O)NC2Cc3ccc(c(Cl)c3)Oc3cc4cc(c3O)Oc3ccc(cc3Cl)C(O)C3NC(=O)C(NC(=O)C4NC(=O)C(NC2=O)c2cc(O)cc(c2)Oc2cc1ccc2O)c1ccc(O)c(c1)-c1c(O)cc(O)cc1C(C(=O)O)NC3=O 0.0
		compound number:  116 
			SMILE:  CN[C@H](CC(C)C)C(=O)N[C@@H]1C(=O)N[C@@H](CC(N)=O)C(=O)N[C@@H]2C(=O)N[C@H]3C(=O)N[C@H](C(=O)N[C@H](C(=O)O)c4cc(O)cc(O)c4-c4cc3ccc4O)[C@@H](O)c3ccc(c(Cl)c3)Oc3cc2cc(c3O[C@@H]2O[C@H](CO)[C@H](O)[C@@H](O)[C@H]2O[C@@H]2C[C@](C)(N)[C@@H](O)[C@H](C)O2)Oc2ccc(cc2Cl)[C@H]1O 0.0
[107, 116]
you have some molecular alerts. It means that these molecules are quite dissimilar
You can (1) eliminate them or (2) maintain them
What is your choice (1/2)?2
Ok, continue with entire dataframe.

NUMBER OF CLUSTERS:  2
	SETS:  {393, 2}
	ALERTS!!
		compound number:  107 
			SMILE:  NC1C(=O)NC2Cc3ccc(c(Cl)c3)Oc3cc4cc(c3O)Oc3ccc(cc3Cl)C(O)C3NC(=O)C(NC(=O)C4NC(=O)C(NC2=O)c2cc(O)cc(c2)Oc2cc1ccc2O)c1ccc(O)c(c1)-c1c(O)cc(O)cc1C(C(=O)O)NC3=O 0.0
		compound number:  116 
			SMILE:  CN[C@H](CC(C)C)C(=O)N[C@@H]1C(=O)N[C@@H](CC(N)=O)C(=O)N[C@@H]2C(=O)N[C@H]3C(=O)N[C@H](C(=O)N[C@H](C(=O)O)c4cc(O)cc(O)c4-c4cc3ccc4O)[C@@H](O)c3ccc(c(Cl)c3)Oc3cc2cc(c3O[C@@H]2O[C@H](CO)[C@H](O)[C@@H](O)[C@H]2O[C@@H]2C[C@](C)(N)[C@@H](O)[C@H](C)O2)Oc2ccc(cc2Cl)[C@H]1O 0.0
[107, 116]
you have some molecular alerts. It means that these molecules are quite dissimilar
You can (1) eliminate them or (2) maintain them
What is your choice (1/2)?2
Ok, continue with entire dataframe.

NUMBER OF CLUSTERS:  1
	SETS:  {395}
	ALERTS!!
[]
     index                                             SMILES    y  C-001  C-002  C-003  C-004  C-005  C-006  ...  NdssS   SssCH2   SsssCH    SdssC    SaasC    SaaaC   SssssC    SLogP  cluster
0        0         CCOC(=O)C1(c2ccccc2)CCN(CCC(O)c2ccccc2)CC1  0.0    1.0    3.0    0.0    1.0    0.0    4.0  ...    0.0  5.07431 -0.44716 -0.11095  2.00656  0.00000 -0.54979  3.70700        0
1        1  N=C1N[C@@H](O)C[C@H]([C@@H]2NC(=O)/C(=C/NC(N)=...  1.0    0.0    4.0    0.0    0.0    0.0    4.0  ...    0.0 -1.97309 -9.91344 -8.71327  0.00000  0.00000  0.00000 -8.67313        0
2        2                       O=C(NCCN1CCOCC1)c1ccc(Cl)cc1  1.0    0.0    0.0    0.0    0.0    0.0    6.0  ...    0.0  4.96631  0.00000 -0.05562  1.27893  0.00000  0.00000  1.40200        0
3        3                                    O=C(O)P(=O)(O)O  0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...    0.0  0.00000  0.00000 -2.09028  0.00000  0.00000  0.00000 -0.15790        0
4        4  CCO[C@@H](Cc1ccc(OCCc2ccc(OS(C)(=O)=O)cc2)cc1)...  1.0    1.0    2.0    0.0    0.0    1.0    2.0  ...    0.0  1.73565 -0.85517 -0.97673  2.79786  0.00000  0.00000  2.67870        0
..     ...                                                ...  ...    ...    ...    ...    ...    ...    ...  ...    ...      ...      ...      ...      ...      ...      ...      ...      ...
390    390  CC[C@@H](CO)NC(=O)[C@@H]1C=C2c3cccc4[nH]cc(c34...  1.0    1.0    2.0    1.0    0.0    1.0    2.0  ...    0.0  2.39438 -0.03671  1.25869  2.59480  2.45246  0.00000  1.92470        0
391    391                          CN(C)C(=O)Oc1ccc[n+](C)c1  0.0    0.0    0.0    0.0    0.0    3.0    0.0  ...    0.0  0.00000  0.00000 -0.36926  0.54509  0.00000  0.00000  0.57150        0
392    392  CC(C)(C)NC(=O)C1CCC2C3CC=C4C=C(C(=O)O)CCC4(C)C...  1.0    5.0    7.0    4.0    2.0    0.0    0.0  ...    0.0  7.06131  1.94304  1.29104  0.00000  0.00000 -0.00114  5.10100        0
393    393  O=C(Nc1cccc2c(=O)cc(-c3nn[nH]n3)oc12)c1ccc(OCC...  0.0    0.0    3.0    0.0    0.0    0.0    1.0  ...    0.0  3.61079  0.00000 -0.34303  3.11975  0.55987  0.00000  4.62710        0
394    394                             CC(C)C[C@H](CN)CC(=O)O  1.0    2.0    2.0    2.0    0.0    0.0    1.0  ...    0.0  1.58528  0.67630 -0.74898  0.00000  0.00000  0.00000  1.08210        0

[395 rows x 880 columns]
0
cluster0
     index                                             SMILES    y  C-001  C-002  C-003  C-004  C-005  C-006  ...  NdssS   SssCH2   SsssCH    SdssC    SaasC    SaaaC   SssssC    SLogP  cluster
0        0         CCOC(=O)C1(c2ccccc2)CCN(CCC(O)c2ccccc2)CC1  0.0    1.0    3.0    0.0    1.0    0.0    4.0  ...    0.0  5.07431 -0.44716 -0.11095  2.00656  0.00000 -0.54979  3.70700        0
1        1  N=C1N[C@@H](O)C[C@H]([C@@H]2NC(=O)/C(=C/NC(N)=...  1.0    0.0    4.0    0.0    0.0    0.0    4.0  ...    0.0 -1.97309 -9.91344 -8.71327  0.00000  0.00000  0.00000 -8.67313        0
2        2                       O=C(NCCN1CCOCC1)c1ccc(Cl)cc1  1.0    0.0    0.0    0.0    0.0    0.0    6.0  ...    0.0  4.96631  0.00000 -0.05562  1.27893  0.00000  0.00000  1.40200        0
3        3                                    O=C(O)P(=O)(O)O  0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...    0.0  0.00000  0.00000 -2.09028  0.00000  0.00000  0.00000 -0.15790        0
4        4  CCO[C@@H](Cc1ccc(OCCc2ccc(OS(C)(=O)=O)cc2)cc1)...  1.0    1.0    2.0    0.0    0.0    1.0    2.0  ...    0.0  1.73565 -0.85517 -0.97673  2.79786  0.00000  0.00000  2.67870        0
..     ...                                                ...  ...    ...    ...    ...    ...    ...    ...  ...    ...      ...      ...      ...      ...      ...      ...      ...      ...
390    390  CC[C@@H](CO)NC(=O)[C@@H]1C=C2c3cccc4[nH]cc(c34...  1.0    1.0    2.0    1.0    0.0    1.0    2.0  ...    0.0  2.39438 -0.03671  1.25869  2.59480  2.45246  0.00000  1.92470        0
391    391                          CN(C)C(=O)Oc1ccc[n+](C)c1  0.0    0.0    0.0    0.0    0.0    3.0    0.0  ...    0.0  0.00000  0.00000 -0.36926  0.54509  0.00000  0.00000  0.57150        0
392    392  CC(C)(C)NC(=O)C1CCC2C3CC=C4C=C(C(=O)O)CCC4(C)C...  1.0    5.0    7.0    4.0    2.0    0.0    0.0  ...    0.0  7.06131  1.94304  1.29104  0.00000  0.00000 -0.00114  5.10100        0
393    393  O=C(Nc1cccc2c(=O)cc(-c3nn[nH]n3)oc12)c1ccc(OCC...  0.0    0.0    3.0    0.0    0.0    0.0    1.0  ...    0.0  3.61079  0.00000 -0.34303  3.11975  0.55987  0.00000  4.62710        0
394    394                             CC(C)C[C@H](CN)CC(=O)O  1.0    2.0    2.0    2.0    0.0    0.0    1.0  ...    0.0  1.58528  0.67630 -0.74898  0.00000  0.00000  0.00000  1.08210        0

[395 rows x 880 columns]
     index                                             SMILES    y  C-001  C-002  C-003  C-004  C-005  C-006  ...  NdssS   SssCH2    SsssCH    SdssC    SaasC    SaaaC   SssssC   SLogP  cluster
205    205  CN(CCCCCCN(C)C(=O)Oc1ccc[n+](C)c1)C(=O)Oc1ccc[...  0.0    0.0    4.0    0.0    0.0    4.0    2.0  ...    0.0  4.94219   0.00000 -0.72594  1.04820  0.00000  0.00000  2.4574        0
153    153  NC1[C@@H]2CN(c3nc4c(cc3F)c(=O)c(C(=O)O)cn4-c3c...  1.0    0.0    0.0    2.0    0.0    0.0    2.0  ...    0.0  1.00716   0.56521 -1.55901 -3.53951 -0.44656  0.00000  1.8945        0
221    221                    CN1C(=O)C[C@@](C)(c2ccccc2)C1=O  1.0    1.0    1.0    0.0    1.0    1.0    0.0  ...    0.0  0.27019   0.00000 -0.21754  0.90991  0.00000 -0.67352  1.3330        0
225    225        CCN1CCC[C@H]1CNC(=O)c1cc(S(=O)(=O)CC)ccc1OC  1.0    2.0    2.0    0.0    0.0    1.0    4.0  ...    0.0  4.78236   0.34028 -0.30282  0.77907  0.00000  0.00000  1.7029        0
316    316  OC[C@H]1O[C@](O)(CO)[C@@H](O)[C@@H]1O[C@@H]1O[...  0.0    0.0    0.0    0.0    0.0    0.0    3.0  ...    0.0 -2.32024 -12.21238  0.00000  0.00000  0.00000 -2.36886 -5.3956        0

[5 rows x 880 columns]
     index                                             SMILES    y  C-001  C-002  C-003  C-004  C-005  C-006  ...  NdssS   SssCH2    SsssCH    SdssC    SaasC    SaaaC   SssssC   SLogP  cluster
205    205  CN(CCCCCCN(C)C(=O)Oc1ccc[n+](C)c1)C(=O)Oc1ccc[...  0.0    0.0    4.0    0.0    0.0    4.0    2.0  ...    0.0  4.94219   0.00000 -0.72594  1.04820  0.00000  0.00000  2.4574        0
153    153  NC1[C@@H]2CN(c3nc4c(cc3F)c(=O)c(C(=O)O)cn4-c3c...  1.0    0.0    0.0    2.0    0.0    0.0    2.0  ...    0.0  1.00716   0.56521 -1.55901 -3.53951 -0.44656  0.00000  1.8945        0
221    221                    CN1C(=O)C[C@@](C)(c2ccccc2)C1=O  1.0    1.0    1.0    0.0    1.0    1.0    0.0  ...    0.0  0.27019   0.00000 -0.21754  0.90991  0.00000 -0.67352  1.3330        0
225    225        CCN1CCC[C@H]1CNC(=O)c1cc(S(=O)(=O)CC)ccc1OC  1.0    2.0    2.0    0.0    0.0    1.0    4.0  ...    0.0  4.78236   0.34028 -0.30282  0.77907  0.00000  0.00000  1.7029        0
316    316  OC[C@H]1O[C@](O)(CO)[C@@H](O)[C@@H]1O[C@@H]1O[...  0.0    0.0    0.0    0.0    0.0    0.0    3.0  ...    0.0 -2.32024 -12.21238  0.00000  0.00000  0.00000 -2.36886 -5.3956        0

[5 rows x 880 columns]
Train set contains:
	114 negative values
	182 positive values
	ratio neg / pos: 0.6263736263736264
Test set contains:
	38 negative values
	61 positive values
	ratio neg / pos: 0.6229508196721312
If you find this imbalanced, try to decomment line 44 of split_by_kmeans.py module. It can give an error! 

The following files have been created:

/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-cleaned_from_kmeans.csv
/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-train_set.csv
/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-test_set.csv

Do you want to perform any other step?(y/n):  
######################### MAIN MENU #########################

Please select what do you want to do: 
[01] Elimination of 3D descriptors [your dataset will be saved as [Name]_no3D]
[1] "y" transformation + dataset random order + Knn imputation
[2] Initial feature reduction: infinite, correlated, constant and empty values
[3] Generation of train and test sets based in kmeans
[4] Descriptor standarization
[5] Feature selection by RFE
[6] Feature selection by FI based on LGBM
[7] Feature selection by Permutation importance
[8] Select own features (inside the script)
[0] Exit NEO

Your choice: 4
[+] Descriptor standarization

Please select the method to standarize the descriptors: 
[1] StandardScaler
[2] MinMaxScaler
Your choice (1/2)?: 1
Two files located in "/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/" folder are needed
These files must be called:
	"TK_HIA_us-train_set.csv"
	"TK_HIA_us-test_set.csv"
Continue (Y/n)?

The following files have been created:

/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-stand_train_set.csv
/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-stand_test_set.csv
/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-alldataset.sca

Do you want to perform any other step?(y/n):  

######################### MAIN MENU #########################

Please select what do you want to do: 
[01] Elimination of 3D descriptors [your dataset will be saved as [Name]_no3D]
[1] "y" transformation + dataset random order + Knn imputation
[2] Initial feature reduction: infinite, correlated, constant and empty values
[3] Generation of train and test sets based in kmeans
[4] Descriptor standarization
[5] Feature selection by RFE
[6] Feature selection by FI based on LGBM
[7] Feature selection by Permutation importance
[8] Select own features (inside the script)
[0] Exit NEO

Your choice: 6
[+] Reduction by FI based on lgbm
Two files located in "/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/" folder are needed
These files must be called:
	"TK_HIA_us-stand_train_set.csv"
	"TK_HIA_us-stand_test_set.csv"
Continue (Y/n)?

Please define if your model is for [1] classification or [2] regression:1

Please define your parameters for lgbm selection for classification parameters: 
eval_metric (l2/auc/binary_logloss):l2
Training Gradient Boosting Model

Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[41]	valid_0's l2: 0.156009	valid_0's binary_logloss: 0.482558
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[65]	valid_0's l2: 0.0993309	valid_0's binary_logloss: 0.340854
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[55]	valid_0's l2: 0.142398	valid_0's binary_logloss: 0.46876
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[96]	valid_0's l2: 0.133725	valid_0's binary_logloss: 0.447463
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[55]	valid_0's l2: 0.153115	valid_0's binary_logloss: 0.48895
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[9]	valid_0's l2: 0.210589	valid_0's binary_logloss: 0.608869
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[115]	valid_0's l2: 0.0957131	valid_0's binary_logloss: 0.310669
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[120]	valid_0's l2: 0.128482	valid_0's binary_logloss: 0.402152
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[68]	valid_0's l2: 0.108265	valid_0's binary_logloss: 0.349011
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[60]	valid_0's l2: 0.0969977	valid_0's binary_logloss: 0.323703

436 features with zero importance after one-hot encoding.

Please, close the plots to continue...
251 features required for 0.90 of cumulative importance
Please, close the plots to continue...
250 features required for cumulative importance of 0.90 after one hot encoding.
626 features do not contribute to cumulative importance of 0.90.

                        feature  importance  normalized_importance  cumulative_importance
0                       AATSC1c        20.7               0.033078               0.033078
1                       GATS1se        20.6               0.032918               0.065996
2                         N-079        14.9               0.023810               0.089805
3                        GATS7d        12.0               0.019175               0.108981
4                          SIC3        11.7               0.018696               0.127677
..                          ...         ...                    ...                    ...
578                       C-029         0.0               0.000000               1.000000
577                  SLogP_VSA8         0.0               0.000000               1.000000
576                       C-018         0.0               0.000000               1.000000
587                       C-036         0.0               0.000000               1.000000
875  nR_9_False_True_False_None         0.0               0.000000               1.000000

[876 rows x 4 columns]

The following files have been created:

/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-train_featured_importances.csv

Now you can select a number of features.
Please, indicate the number of selected features: 10

Selected features: 
 ['AATSC1c', 'GATS1se', 'N-079', 'GATS7d', 'SIC3', 'SLogP_VSA2', 'AATSC0c', 'JGI7', 'AATSC3c', 'ATSC3c']
Size of the database, preimputation: (296, 10)
[+] fitting
[+] transforming
Size of the database, postimputation: (296, 10)

The following files have been created:

/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-train_reduction_GBM.csv
/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-test_reduction_GBM.csv
/home/lauri/Desktop/ProtoQSAR/PROJECTS/contratas/IRB/Models/TK_HIA_us/TK_HIA_us-train_set.sca
train_set (296, 12)
test_set (99, 12) 


Do you agree with that number of features?(y/n): y

Do you want to perform any other step?(y/n):  n

Thanks for using NEO!

